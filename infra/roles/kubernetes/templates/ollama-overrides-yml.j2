# ollama_helm_version: {{ ollama_helm_version }}

# Ollama parameters
ollama:
  gpu:
    # -- Enable GPU integration
    enabled: {{ ollama_gpu_enabled }}

    # -- GPU type: 'nvidia' or 'amd'
    type: 'nvidia'

    # -- Specify the number of GPU
    number: {{ ollama_number_of_gpus }}

  # -- List of models to pull at container startup
  models: 
    - {{ ollama_default_model }}

  # -- Default model to serve, if not set, no model will be served at container startup
  defaultModel: "{{ ollama_default_model }}"

# -- Specify runtime class
runtimeClassName: "nvidia"

ingress:
  enabled: true
  hosts:
  - host: ollama.{{ ollama_tld }}
    paths:
      - path: /
        pathType: Prefix  
  - host: ollama.ollama.{{ ollama_cluster_name }}.{{ ollama_tld }}
    paths:
      - path: /
        pathType: Prefix

image:
  tag: {{ ollama_helm_image_tag }}